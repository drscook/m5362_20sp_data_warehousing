{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PD4SDG_preprocessingV3ipynb","provenance":[{"file_id":"1rC6m3B34wVKoDMub6ToJDaEoGdVh_9qL","timestamp":1586293152898},{"file_id":"1CrgxmxdElSEcZ5nPkhVUIdesdV_KCyvp","timestamp":1585661717594}],"collapsed_sections":["Yd3ELKSIhw3J","hBWH1ASzhjrN"],"mount_file_id":"1rC6m3B34wVKoDMub6ToJDaEoGdVh_9qL","authorship_tag":"ABX9TyOuoGOyLNoYYpID5cWxF2bw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eHx7PQtqJiTt","colab_type":"code","colab":{}},"source":["# Make sure we have the new pandas 1.0 (Jan 2020) so we can use the new string dtype\n","! pip install pandas --upgrade\n","\n","# Fuzzy text matching\n","! pip install \"textdistance[extras]\" --upgrade\n","\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', None)\n","\n","path = '/content/drive/My Drive/active/m5362_20sp_data_warehousing/PD4SDG/PD4SDG_new/data sources'\n","\n","def formatter(X):\n","    \"\"\"Common cleaning and formatting\"\"\"\n","    # convert to modern dtypes like \"string\" introduced in Pandas 1.0 (Jan 2020)\n","    df = X.copy().convert_dtypes()\n","    \n","    # trim whitespace, replace missing and length 0 or 1 strings with '', standard capitalization\n","    f = lambda x: x.str.strip().fillna('').str.lower() \\\n","        .str.replace(r'^.{0,1}$', '').str.replace(r'^st\\.', 'saint').str.replace(r'^st', 'saint').str.replace('&', 'and') \\\n","        .str.title()\n","    \n","    # format column names, but use lowercase (my preference)\n","    df.columns = f(df.columns).str.lower()\n","    \n","    # format string columns\n","    strings = df.select_dtypes(include=['string','object']).columns   # finds string columns\n","    df[strings] = df[strings].apply(f).astype('string')   # formats them\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fa6pYZRVSHkP","colab_type":"code","colab":{}},"source":["world_raw = pd.read_json(f\"{path}/raw/world-cities.json\").set_index('geonameid')\n","world = formatter(world_raw) \\\n","    .rename(columns={'name':'city'})\n","world = world[cols].drop_duplicates().sort_values(['country', 'subcountry', 'city'])\n","world.to_csv(f\"{path}/city.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cgp33P1xD45","colab_type":"code","colab":{}},"source":["country_data_raw = pd.read_csv(f\"{path}/raw/countries of the world.csv\", decimal=',')\n","country_data = formatter(country_data_raw)\n","country_data.index += 1\n","country_data.index.rename('id', inplace=True)\n","country_data.to_csv(f\"{path}/country_data.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN8gFLRZWM5Z","colab_type":"code","colab":{}},"source":["pd4sdg_raw = pd.read_excel(f\"{path}/raw/PD4SDG.xlsx\")\n","pd4sdg = formatter(pd4sdg_raw) \\\n","    .rename(columns={'project location 1':'site', 'title':'title', 'project_idx':'un_id'})\n","pd4sdg['un_id'] = pd4sdg['un_id'].str.replace(\"'\", \"\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W56mRWg2PmJp","colab_type":"code","colab":{}},"source":["# Get unique un_idx list\n","f = lambda L: max(L, key=len)  # returns longest string in list\n","project = pd4sdg.groupby('un_id').agg(\n","    title = ('title', f),\n","    site = ('site', f),\n","    repeats = ('un_id', 'count'),\n","    ).reset_index()\n","project.index += 1\n","project.index.rename('id', inplace=True)\n","project.to_csv(f\"{path}/project.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pfUgl8QZAYc","colab_type":"code","colab":{}},"source":["# Get project_entity links\n","partners = ['lead partner'] + [f\"partner {i+1}\" for i in range(237)]\n","\n","# Temp list to hold results\n","L = []\n","for (n, c) in enumerate(partners):\n","    # get partner columns, rename cols, drop rows where name is ''\n","    s = pd4sdg[['un_id', c]].rename(columns={c: 'name'})\n","    # s = s[s['name'].str.len() > 0]\n","    s = s[s['name'] != '']\n","\n","    # record partner number on project (in case precedence matters - we don't believe it does)\n","    s['n'] = n\n","\n","    # Append\n","    L.append(s)\n","\n","# concat the lists stored in L\n","project_entity = pd.concat(L)\n","project_entity.index += 1\n","project_entity.index.rename('id', inplace=True)\n","project_entity.to_csv(f\"{path}/project_entity.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yd3ELKSIhw3J","colab_type":"text"},"source":["# Text matching code"]},{"cell_type":"code","metadata":{"id":"EALitxhWNCeK","colab_type":"code","colab":{}},"source":["import textdistance\n","def textdist_func(a, b):\n","    try:\n","        a[0].lower() + b[0].lower()\n","    except:\n","        return 0.0\n","    else:\n","        return textdistance.levenshtein.normalized_similarity(a, b)\n","textdist_ufunc = np.frompyfunc(textdist_func, 2, 1)\n","def textdist(A, B):\n","    return textdist_ufunc.outer(A, B).astype('float')\n","\n","\n","def large(X, k=2, axis=0):\n","    if k <= 1:\n","        srt = np.argmax(X, axis)  # find\n","        srt = np.expand_dims(srt, axis)\n","        X = np.take_along_axis(X, srt, axis)\n","    else:\n","        X = np.rollaxis(X, axis, 0)\n","        srt1 = np.argpartition(X, -k, axis=0)[-k:]\n","        X = np.take_along_axis(X, srt1, axis=0)\n","        srt2 = np.argsort(X, axis=0)[::-1]\n","        X = np.take_along_axis(X, srt2, axis=0)\n","        srt = np.take_along_axis(srt1, srt2, axis=0)\n","        X   = np.rollaxis(X, 0, axis+1)\n","        srt = np.rollaxis(srt, 0, axis+1)\n","    return X, srt\n","\n","\n","def text_match(orig, targ, num_matches=1):\n","    def f(X):\n","        df = pd.DataFrame(X.copy()).drop_duplicates()\n","        m = (df!='').any(axis=1)\n","        return df[m]\n","    orig = f(orig)\n","    targ = f(targ)\n","\n","    all_scores = textdist(orig.to_numpy(), targ.to_numpy())\n","    max_scores = all_scores.max((1,3))\n","    best_score, srt = large(max_scores.T, k=num_matches, axis=0)\n","\n","    index = targ.index.to_numpy()\n","    names = targ.iloc[:, 0].to_numpy()\n","    for i in range(num_matches):\n","        orig[f\"score_{i+1}\"] = (best_score[i]*100).round(1)\n","        orig[f\"index_{i+1}\"] =   index[srt[i]]\n","        orig[ f\"name_{i+1}\"]  =  names[srt[i]]\n","   \n","    return orig, all_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdOghHU77q__","colab_type":"code","colab":{}},"source":["num_matches = 3\n","all_site = pd.read_csv(f\"{path}/output/site.csv\").set_index('site_id')\n","\n","entity   = pd.read_csv(f\"{path}/output/entity.csv\").set_index('id')[['name', 'type', 'site']]\n","s = all_site.index > 5000\n","df, s = text_match(entity['site'][:5], all_site.iloc[s,3:], num_matches)\n","for i in range(num_matches):\n","    df[f\"use_match_{i+1}\"] = ''\n","df[f\"use_match_{1}\"] = 'x'\n","df['no_match_found'] = ''\n","entity_site_fix = entity.join(df, lsuffix='_orig').sort_values('score_1')\n","entity_site_fix.to_csv(f\"{path}/output/entity_site_fix.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hBWH1ASzhjrN","colab_type":"text"},"source":["# Old or Experimental Code"]},{"cell_type":"code","metadata":{"id":"fY3pMGW-NntA","colab_type":"code","colab":{}},"source":["## Read and process entity data\n","entity = pd.read_csv(f\"{path}/output/entity.csv\")\n","\n","\n","check_sites = True\n","look_for_duplicates = True\n","num_matches = 3\n","\n","entity_data = formatter(pd.read_excel(f\"{path}/raw/entity_data.xlsx\"))\n","missing_idx = (entity_data[['name','type','city','country']]=='').any(axis=1)\n","if missing_idx.any():\n","    display(entity_data[missing_idx])\n","    raise Exception('Missing data in entity_data.xlsx')\n","entity_data['site'] = cat(entity_data[['city', 'subcountry', 'country']])\n","\n","if check_sites:\n","    site = pd.concat([entity_data['site'], project['site']]).drop_duplicates().reset_index(drop=True)\n","    site_fix, s = text_match(site, world_site, num_matches)\n","    site_fix.sort_values('score_1', ascending=True, inplace=True)\n","\n","    for i in range(num_matches):\n","        entity_data_similarity[f\"use_match_{i+1}\"] = ''\n","\n","    write_file(entity_data_similarity, 'entity_data_similarity')\n","\n","\n","# idx = proj_site_fix['score_1'] < 100\n","# proj_site_fix = proj_site_fix.loc[idx]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vcmVLG0x8Y2t","colab_type":"code","colab":{}},"source":["entity_data = formatter(pd.read_excel(f\"{path}/raw/entity_data.xlsx\"))\n","missing_idx = (entity_data[['name','type','city','country']]=='').any(axis=1)\n","if missing_idx.any():\n","    display(entity_data[missing_idx])\n","    raise Exception('Missing data in entity_data.xlsx')\n","entity_data['type']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gDnTvE_kq_8","colab_type":"code","colab":{}},"source":["world_raw = pd.read_json(f\"{path}/raw/world-cities.json\").set_index('geonameid')\n","cols = ['city', 'subcountry', 'country']\n","world = formatter(world_raw) \\\n","    .rename(columns={'name':'city'})\n","\n","world = world[cols].drop_duplicates().sort_values(cols[::-1])\n","\n","F = [f\"form_{i+1}\" for i in range(4)]\n","for f in F:\n","    world[f] = ''\n","\n","def cat(df):\n","        return df.add(\", \").sum(axis=1).str.strip(\", \")\n","\n","df = world.copy()\n","df[['city', 'subcountry']] = ''\n","df2 = df.iloc[:2].copy()\n","df2['country'] = ['Global', 'European Union']\n","df = df2.append(df)\n","df[F[0]] = cat(df[['country']])\n","country_df = df.drop_duplicates().reset_index(drop=True)\n","\n","\n","df = world.copy()\n","df['city'] = ''\n","df[F[0]] = cat(df[['subcountry', 'country']])\n","df[F[1]] = cat(df[['subcountry']])\n","subcountry_df = df.drop_duplicates().reset_index(drop=True)\n","subcountry_df.index += 1000\n","pd.\n","\n","\n","\n","df = world.copy()\n","df[F[0]] = cat(df[['city', 'subcountry', 'country']])\n","df[F[1]] = cat(df[['city',               'country']])\n","df[F[2]] = cat(df[['city', 'subcountry'           ]])\n","df[F[3]] = cat(df[['city'                         ]])\n","city_df = df.copy()\n","\n","\n","df = world.copy()\n","df['city'] = ''\n","df[F[0]] = cat(df[['subcountry', 'country']])\n","df[F[1]] = cat(df[['subcountry']])\n","subcountry_df = df.copy().reset_index(drop=True)\n","subcountry_df.index += 1000\n","\n","\n","df = world.copy()\n","df[['city', 'subcountry']] = ''\n","df2 = df.iloc[:2].copy()\n","df2['country'] = ['Global', 'European Union']\n","df = df2.append(df)\n","df[F[0]] = cat(df[['country']])\n","country_df = df.copy().reset_index(drop=True)\n","\n","\n","world = pd.concat([country_df, subcountry_df]).drop_duplicates().reset_index(drop=True)\n","world = pd.concat([world, city_df]).drop_duplicates()\n","world.index.rename('geonameid', inplace=True)\n","\n","world_site = world[F]\n","\n","with pd.ExcelWriter(f'{path}/world.xlsx') as writer:  \n","    world_raw.to_excel(writer, sheet_name='raw')\n","    world.to_excel(writer, sheet_name='clean')\n","    world_site.to_excel(writer, sheet_name='sites')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wS6nDjysWR5Q","colab_type":"code","colab":{}},"source":["# do this in sql\n","\n","# name fix to make country names in country dataset compatible with country names in the city dataset\n","rep =  {\n","    'Bahamas, The': 'Bahamas',\n","    'British Virgin Is.': 'British Virgin Islands',\n","    'Burma': 'Myanmar',\n","    'Central African Rep.': 'Central African Republic',\n","    'Congo, Dem. Rep.': 'Democratic Republic Of The Congo',\n","    'Congo, Repub. Of The': 'Republic Of The Congo',\n","    \"Cote D'Ivoire\": 'Ivory Coast',\n","    'Gambia, The': 'Gambia',\n","    # 'Gaza Strip':\n","    'Korea, North': 'North Korea',\n","    'Korea, South': 'South Korea',\n","    'Macau': 'Macao',\n","    'Micronesia, Fed. St.': 'Micronesia',\n","    'N. Mariana Islands': 'Northern Mariana Islands',\n","    # 'Netherlands Antilles':\n","    'Turks And Caicos Is': 'Turks And Caicos Islands',\n","    'Virgin Islands': 'U.S. Virgin Islands',\n","    # 'West Bank':\n","}\n","country_data_raw = pd.read_csv(f\"{path}/raw/countries of the world.csv\", decimal=',')\n","country_data = formatter(country_data_raw)\n","country_data.index += 1\n","country_data.index.rename('id', inplace=True)\n","\n","country_data.to_csv(f\"{path}/raw/countries of the world cleaned.csv\")\n","\n","# country_data['country'] = country_data['country'].replace(rep).astype('string')\n","# country_data.set_index('country', inplace=True)\n","\n","# # Create \"Palestine\" to be compatible with city dataset\n","# def combine(old):\n","#     x = country_data.loc[old]\n","#     y = x.iloc[0].copy()\n","#     y[1:3] = x.iloc[:,1:3].sum().astype(int)\n","#     y[3] = np.round(y[1] / y[2], 2)\n","#     y[4:] = np.round(x.iloc[:,4:].mean(), 2)\n","#     return y\n","\n","# pal = ['Gaza Strip','West Bank']\n","# country_data.loc['Palestinian Territory'] = combine(pal)\n","\n","# with pd.ExcelWriter(f'{path}/country_data.xlsx') as writer:  \n","#     country_data_raw.to_excel(writer, sheet_name='raw')\n","#     country_data.to_excel(writer, sheet_name='clean')\n","country_data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwFmSNMZ8drm","colab_type":"code","colab":{}},"source":["entity_data['type'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SnG5BrL2tOKJ","colab_type":"code","colab":{}},"source":["## Read and process entity data\n","look_for_duplicates = True\n","num_matches = 3\n","\n","entity_data = formatter(pd.read_excel(f\"{path}/raw/entity_data.xlsx\"))\n","missing_idx = (entity_data[['name','type','city','country']]=='').any(axis=1)\n","if missing_idx.any():\n","    display(entity_data[missing_idx])\n","    raise Exception('Missing data in entity_data.xlsx')\n","\n","if look_for_duplicates:\n","    A = entity_data['name'][:100]\n","    B = A\n","    left, s = text_match(A, B, num_matches+1)\n","    # display(left.head())\n","    # idx = left['score_1'] > 0.8\n","    # left = left.loc[idx]\n","\n","    attr = ['score', 'index', 'name']\n","    srt = [f\"{s}_{i}\" for i in range(1, num_matches+1) for s in attr]\n","    entity_data_similarity = left[srt].sort_values('score_1', ascending=False)\n","\n","    entity_data_similarity['keep_orig'] = 'x'\n","    for i in range(num_matches):\n","        entity_data_similarity[f\"use_match_{i+1}\"] = ''\n","\n","    write_file(entity_data_similarity, 'entity_data_similarity')\n","# entity_data_similarity.head()\n","entity_data_similarity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8LlitDwNS3H","colab_type":"code","colab":{}},"source":["entity_data.head()\n","\n","# sites = project['site']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"45116gb_Ww7q","colab_type":"code","colab":{}},"source":["project['site']\n","\n","\n","proj_site_fix, s = text_match(project['site'].drop_duplicates(), world_site, num_matches=3)\n","proj_site_fix.sort_values('score_1', ascending=False, inplace=True)\n","idx = proj_site_fix['score_1'] < 100\n","proj_site_fix = proj_site_fix.loc[idx]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uv6ktOPKJ-RG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzRoUtN1lsb1","colab_type":"code","colab":{}},"source":["world_raw = pd.read_json(f\"{path}/raw/world-cities_json.json\").set_index('geonameid').drop_duplicates()\n","\n","cols = ['city', 'subcountry', 'country']\n","world = formatter(world_raw) \\\n","    .rename(columns={'name':'city'})\n","world = world[cols].sort_values(cols[::-1])\n","\n","def cat(df):\n","        return df.add(\", \").sum(axis=1).str.strip(\", \")\n","\n","A = pd.DataFrame()\n","A[F[0]] = cat(world[['city', 'subcountry', 'country']])\n","A[F[1]] = cat(world[['city',               'country']])\n","A[F[2]] = cat(world[['city', 'subcountry'           ]])\n","A[F[3]] = cat(world[['city'                         ]])\n","\n","B = pd.DataFrame()\n","B[F[0]] = cat(world[['subcountry', 'country']])\n","B[F[1]] = cat(world[[              'country']])\n","\n","C = pd.DataFrame()\n","C[F[0]] = cat(world[['country']])\n","\n","D = C.iloc[:2].copy()\n","D[F[0]] = ['Global', 'European Union']\n","\n","world_site = pd.concat([D, C, B]).reset_index(drop=True)\n","world_site = pd.concat([world_site, A]).drop_duplicates()\n","\n","world_site.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRE2NROtPSw4","colab_type":"code","colab":{}},"source":["world_raw = pd.read_json(f\"{path}/raw/world-cities_json.json\").set_index('geonameid')\n","cols = ['city', 'subcountry', 'country']\n","world = formatter(world_raw) \\\n","    .rename(columns={'name':'city'})\n","world = world[cols].sort_values(cols[::-1])#['country', 'subcountry', 'city'])\n","\n","F = [f\"form_{i+1}\" for i in range(4)]\n","for f in F:\n","    world[f] = ''\n","\n","def cat(df):\n","        return df.add(\", \").sum(axis=1).str.strip(\", \")\n","\n","A = world.copy()\n","A[F[0]] = cat(A[cols])\n","A[F[1]] = cat(A[['city', 'country']])\n","A[F[2]] = cat(A[['city', 'subcountry']])\n","A[F[3]] = cat(A[['city']])\n","\n","\n","B = world.copy()\n","B['city'] = ''\n","B = B.drop_duplicates()\n","B[F[0]] = cat(B[['subcountry', 'country']])\n","B[F[1]] = cat(B[['subcountry']])\n","\n","C = world.copy()\n","C[['city', 'subcountry']] = ''\n","D = C.iloc[:2].copy()\n","D['country'] = ['Global', 'European Union']\n","C = D.append(C).drop_duplicates()\n","C[F[0]] = cat(C[['country']])\n","\n","D = pd.concat([C, B], ignore_index=True)\n","world = pd.concat([D, A])\n","world.index.rename('geonameid', inplace=True)\n","\n","\n","world.head()\n","# world.tail()\n","\n","# A = world.copy()\n","# A['city'] = ''\n","# A = A.drop_duplicates()\n","\n","\n","# B = A.copy()\n","# B['subcountry'] = ''\n","# B = B.drop_duplicates()\n","\n","# C = B.iloc[:2].copy()\n","# C['country'] = ['Global', 'European Union']\n","\n","# D = pd.concat([C, B, A], ignore_index=True)\n","# world = pd.concat([D, world])\n","# world.index.rename('geonameid', inplace=True)\n","\n","# with pd.ExcelWriter(f'{path}/world.xlsx') as writer:  \n","#     world_raw.to_excel(writer, sheet_name='raw')\n","#     world.to_excel(writer, sheet_name='clean')\n","#     L = {'site':['city','subcountry','country'],\n","#         'nocountry':['city','subcountry'],\n","#         'nosubcountry':['city','country'],\n","#         'nocity':['subcountry','country'],\n","#         'city':['city'],\n","#         'subcountry':['subcountry'],\n","#         'country':['country'],\n","#     }\n","#     world_mini = dict()\n","#     for lev, cols in L.items():\n","#         X = world[cols].add(\", \").sum(axis=1).str.strip(\", \").drop_duplicates().sort_index()\n","#         X = X[X != '']\n","#         world_mini[lev] = X\n","#         X.to_excel(writer, sheet_name=lev)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5BgKHjrXSeJ","colab_type":"code","colab":{}},"source":["entity_raw = project_entity_raw.drop(columns=['un_idx', 'n']).drop_duplicates()\\\n","    .sort_values(['name', 'country', 'city']).rename(columns={'name':'name_orig'})\n","\n","# apply entity_fix\n","entity_fix = formatter(pd.read_excel(f\"{path}/raw/entity_fix.xlsx\"))\n","entity = pd.merge(entity_raw, entity_fix, how='left', on='name_orig')\n","idx = entity['name'].isna()\n","entity.loc[idx, 'name'] = entity.loc[idx, 'name_orig']\n","\n","# merge entity_data\n","entity = pd.merge(entity, entity_data, how='left', on='name', suffixes=('_orig',''))\n","\n","# look for unmatched entities\n","idx = entity['type'].isna()\n","if idx.any():\n","    entity_nomatch = entity.loc[idx,:'country_orig'].drop_duplicates()\n","\n","    num_matches = 3\n","    A = entity_nomatch['name_orig']\n","    B = entity_data['name']\n","    left, s = text_match(A, B, num_matches)\n","\n","    attr = ['type', 'city', 'subcountry', 'country']\n","    for i in range(num_matches):\n","        attr_new = [f\"{s}_{i}\" for s in attr]\n","        right = entity_data[attr].rename(columns=dict(zip(attr, attr_new)))\n","        left = left.join(right, on=f\"index_{i}\", how='left')\n","\n","    attr = ['score', 'index', 'name'] + attr\n","    srt = [f\"{s}_{i}\"   for i in range(num_matches) for s in attr]\n","    ren = [f\"{s}_{i+1}\" for i in range(num_matches) for s in attr]\n","    entity_nomatch_similarity = left[srt].rename(columns=dict(zip(srt,ren))).sort_values('score_1', ascending=False)\n","\n","    entity_nomatch_similarity['use_orig'] = ''\n","    for i in range(num_matches):\n","        entity_nomatch_similarity[f'use_match_{i+1}'] = ''\n","\n","    for s in attr[2:]:\n","        entity_nomatch_similarity[f'{s}_new'] = ''\n","\n","write_file(entity_nomatch_similarity, 'entity_nomatch_similarity')\n","entity_nomatch_similarity.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ys5ouQjVWcm_","colab_type":"code","colab":{}},"source":["world_raw = pd.read_json(f\"{path}/raw/world-cities_json.json\").set_index('geonameid')\n","world = formatter(world_raw) \\\n","    .rename(columns={'name':'city'}) \\\n","    [['city', 'subcountry', 'country']] \\\n","    .sort_values(['country', 'subcountry', 'city']) \\\n","\n","F = [f\"form_{i+1}\" for i in range(4)]\n","world[F] = ''\n","\n","L = {'site':['city','subcountry','country'],\n","    'nocountry':['city','subcountry'],\n","    'nosubcountry':['city','country'],\n","    'nocity':['subcountry','country'],\n","    'city':['city'],\n","    'subcountry':['subcountry'],\n","    'country':['country'],\n","}\n","D = dict()\n","for lev, cols in L.items():\n","    X = world[cols].add(\", \").sum(axis=1).str.strip(\", \").drop_duplicates()#.sort_index()\n","    X = X[X != '']\n","    d_mini[lev] = X\n","world = pd.concat([d for  in world_mini.values()]).drop_duplicates()\n","    \n","    \n","    X.to_excel(writer, sheet_name=lev)\n","\n","\n","with pd.ExcelWriter(f'{path}/world.xlsx') as writer:  \n","    world_raw.to_excel(writer, sheet_name='raw')\n","    world.to_excel(writer, sheet_name='clean')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJAFm8IRVnxP","colab_type":"code","colab":{}},"source":["num_matches = 2\n","A = project['site'].drop_duplicates()\n","A = A[A != '']\n","B = world_mini['site']\n","left, s = text_match(A, B, num_matches)\n","\n","project_site_similarity = left\n","project_site_similarity.head()\n","# project_site_fix_dict = fix_site(project['site'].to_numpy())\n","# project_site_fix = pd.DataFrame.from_dict(project_site_fix_dict, orient='index')\n","# write_file(project_site_fix, 'project_site_fix')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hShBqdzZXZeL","colab_type":"code","colab":{}},"source":["project_site_similarity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bHVfZALiSGUl","colab_type":"code","colab":{}},"source":["def fix_site(orig, num_matches=1):\n","    B = world_mini['site']\n","    left, s = text_match(orig, B, num_matches)\n","\n","    \n","    # display(left.head(10))\n","\n","\n","    # attr = ['name']\n","    # for i in range(num_matches):\n","    #     attr_new = [f\"{s}_{i}\" for s in attr]\n","    #     right = entity_data[attr].rename(columns=dict(zip(attr, attr_new)))\n","    #     left = left.join(right, on=f\"index_{i}\", how='left')\n","\n","\n","\n","\n","    # site = {s:{'scores':[0.0 for _ in range(n)], 'matches':[None for _ in range(n)]} for s in orig}\n","    \n","    # # iterate over sites\n","    # for lev, w in world_mini.items():\n","    #     d = textdist(orig, w)\n","        \n","    #     for s, val in site.items():\n","            \n","\n","    #     # iterate over known world location in multiple levels like city, subcountry, country and subset of these 3 items\n","        \n","\n","    #         # compute distance scores & find best\n","    #         score = textdist(np.array(w), s)\n","\n","\n","    #         newus_scorus = score.max()\n","\n","    #         # if there is a better score than the current best, we record this improved match\n","    #         if newus_scorus > biggus_scorus:\n","    #             biggus_scorus = newus_scorus\n","    #             site[s]['score'] = biggus_scorus\n","    #             hits = w[score==biggus_scorus].index\n","    #             n = len(hits)\n","    #             site[s]['n'] = n\n","    #             match = world.loc[hits, ['city','subcountry', 'country']]\n","    #             if lev in ['subcountry', 'country', ' nocity']:\n","    #                 match['city'] = ''\n","    #                 if lev in ['country']:\n","    #                     match['subcountry'] = ''\n","    #             site[s]['match'] = match.to_dict(orient='records')\n","    # return site\n","\n","# project_site_fix_dict = fix_site(project['site'].to_numpy())\n","# project_site_fix = pd.DataFrame.from_dict(project_site_fix_dict, orient='index')\n","# write_file(project_site_fix, 'project_site_fix')\n","\n","\n","project_site_fix_dict = fix_site(project['site'].to_numpy())\n","project_site_fix = pd.DataFrame.from_dict(project_site_fix_dict, orient='index')\n","write_file(project_site_fix, 'project_site_fix')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fd6-y95Q6SR7","colab_type":"code","colab":{}},"source":["entity_raw = project_entity_raw.drop(columns=['un_idx', 'n']).drop_duplicates().sort_values(['name', 'country', 'city'])\n","# entity_raw['subcountry'] = ''\n","entity_raw = entity_raw[['name','type','city','country']].rename(columns={'name':'name_orig'})\n","\n","\n","\n","entity_fix = formatter(pd.read_excel(f\"{path}/raw/entity_fix.xlsx\"))\n","entity_data = formatter(pd.read_excel(f\"{path}/raw/entity_data.xlsx\"))\n","missing_idx = (entity_data[['name','type','city','country']]=='').any(axis=1)\n","if missing_idx.any():\n","    display(entity_data[missing_idx])\n","    raise Exception('Missing data in entity_data.xlsx')\n","\n","\n","entity = pd.merge(entity_raw, entity_fix, how='left', on='name_orig')\n","\n","idx = entity['name'].isna()\n","entity.loc[idx, 'name'] = entity.loc[idx, 'name_orig']\n","\n","entity = pd.merge(entity, entity_data, how='left', on='name', suffixes=('_orig',''))\n","\n","idx = entity['type'].isna()\n","entity_nomatch = entity.loc[idx,:'country_orig'].drop_duplicates()\n","\n","num_matches=3\n","fix, _ = text_match(entity_nomatch['name_orig'], entity_data['name'], num_matches)\n","for i in range(num_matches):\n","    fix = fix.join(entity_data.set_index('name'), how='left', on=f\"name_{i}\")\n","    L = ['type', 'city', 'subcountry', 'country']\n","    fix.rename(columns={c:f\"{c}_{i}\" for c in L}, inplace=True)\n","\n","L = ['score', 'name'] + L\n","fix = fix[[f\"{attr}_{i}\" for i in range(num_matches) for attr in L]]\n","fix.sort_values('score_0', ascending=False, inplace=True)\n","\n","entity_nomatch_fix = entity_nomatch.join(fix, on='name_orig', how='left').sort_values('score_0', ascending=False)\n","\n","\n","entity_nomatch_fix['use_orig'] = ''\n","for i in range(num_matches):\n","    entity_nomatch_fix[f'use_match_{i}'] = ''\n","\n","for c in L[1:]:\n","    entity_nomatch_fix[f'{c}_new'] = ''\n","\n","write_file(entity_nomatch_fix, 'entity_nomatch_fix')\n","entity_nomatch_fix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rTomHGAebJe","colab_type":"code","colab":{}},"source":["entity_raw = project_entity_raw.drop(columns=['un_idx', 'n']).drop_duplicates()\\\n","    .sort_values(['name', 'country', 'city']).rename(columns={'name':'name_orig'})\n","\n","# apply entity_fix\n","entity_fix = formatter(pd.read_excel(f\"{path}/raw/entity_fix.xlsx\"))\n","entity = pd.merge(entity_raw, entity_fix, how='left', on='name_orig')\n","\n","\n","idx = entity['name'].isna()\n","entity.loc[idx, 'name'] = entity.loc[idx, 'name_orig']\n","entity = pd.merge(entity, entity_data, how='left', on='name', suffixes=('_orig',''))\n","\n","\n","entity.iloc[6120:6140]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJVAnbUWu4oD","colab_type":"code","colab":{}},"source":["entity_fix.tail()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7dmKeDBV2IQ","colab_type":"code","colab":{}},"source":["entity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNQWjC_up992","colab_type":"code","colab":{}},"source":["entity_nomatch_fix = entity_nomatch.join(fix, on='name_orig', how='left').sort_values('score_0', ascending=False)\n","entity_nomatch_fix['use_orig'] = ''\n","for i in range(num_matches):\n","    entity_nomatch_fix[f'use_match_{i}'] = ''\n","\n","for c in L[1:]:\n","    entity_nomatch_fix[f'{c}_new'] = ''\n","\n","write_file(entity_nomatch_fix, 'entity_nomatch_fix')\n","entity_nomatch_fix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSRDkTxSp1cJ","colab_type":"code","colab":{}},"source":["entity_nomatch.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzLOulkEWlmF","colab_type":"code","colab":{}},"source":["entity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgyfFpBg_DKb","colab_type":"code","colab":{}},"source":["# entity_nomatch.join(df, on='name_orig').sort_values('score_0', ascending=False)\n","num_matches=3\n","i = 0\n","X = df\n","for i in range(num_matches):\n","    X = pd.merge(X, entity_data, left_on=f\"name_{i}\", right_on='name').drop(columns=['name'])\n","    L = ['type', 'city', 'subcountry', 'country']\n","    X.rename(columns={c:f\"{c}_{i}\" for c in L}, inplace=True)\n","\n","X = X[[f\"{attr}_{i}\" for i in range(num_matches) for attr in ['score', 'name', 'type', 'city', 'subcountry', 'country']]]\n","X\n","# for i in range(num_matches):\n","#     X = "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CN1IHBI8G6pv","colab_type":"code","colab":{}},"source":["def fix_site(orig):\n","    # uniquify to avoid wasted effort\n","    orig = np.unique(orig).tolist()\n","\n","    # remove trivial case if present\n","    try:\n","        orig.remove('')\n","    except:\n","        pass\n","\n","    world['site']\n","\n","    # site dict records original, best current score, number of ties for that score, and matches with that score\n","\n","\n","\n","\n","    site = {s:{'score':0.0, 'n':0, 'match':[]} for s in orig}\n","\n","    # chosen distance metric\n","    \n","    \n","    # iterate over sites\n","    for s, val in site.items():\n","        # best current score\n","        biggus_scorus = 0.0\n","\n","        # iterate over known world location in multiple levels like city, subcountry, country and subset of these 3 items\n","        for lev, w in world_mini.items():\n","\n","            # compute distance scores & find best\n","            score = textdist(w.to_numpy(), s)\n","            newus_scorus = score.max()\n","\n","            # if there is a better score than the current best, we record this improved match\n","            if newus_scorus > biggus_scorus:\n","                biggus_scorus = newus_scorus\n","                site[s]['score'] = biggus_scorus\n","                hits = w[score==biggus_scorus].index\n","                n = len(hits)\n","                site[s]['n'] = n\n","                match = world.loc[hits, ['city','subcountry', 'country']]\n","                if lev in ['subcountry', 'country', ' nocity']:\n","                    match['city'] = ''\n","                    if lev in ['country']:\n","                        match['subcountry'] = ''\n","                site[s]['match'] = match.to_dict(orient='records')\n","    return site\n","\n","project_site_fix_dict = fix_site(project['site'].to_numpy())\n","project_site_fix = pd.DataFrame.from_dict(project_site_fix_dict, orient='index')\n","write_file(project_site_fix, 'project_site_fix')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxkrhTd0HZgI","colab_type":"code","colab":{}},"source":["def fix_site(orig):\n","    # uniquify to avoid wasted effort\n","    orig = np.unique(orig).tolist()\n","\n","    # remove trivial case if present\n","    try:\n","        orig.remove('')\n","    except:\n","        pass\n","\n","    # site dict records original, best current score, number of ties for that score, and matches with that score\n","    site = {s:{'score':0.0, 'n':0, 'match':[]} for s in orig}\n","\n","    # chosen distance metric\n","    \n","    \n","    # iterate over sites\n","    for s, val in site.items():\n","        # best current score\n","        biggus_scorus = 0.0\n","\n","        # iterate over known world location in multiple levels like city, subcountry, country and subset of these 3 items\n","        for lev, w in world_mini.items():\n","\n","            # compute distance scores & find best\n","            score = textdist(w.to_numpy(), s)\n","            newus_scorus = score.max()\n","\n","            # if there is a better score than the current best, we record this improved match\n","            if newus_scorus > biggus_scorus:\n","                biggus_scorus = newus_scorus\n","                site[s]['score'] = biggus_scorus\n","                hits = w[score==biggus_scorus].index\n","                n = len(hits)\n","                site[s]['n'] = n\n","                match = world.loc[hits, ['city','subcountry', 'country']]\n","                if lev in ['subcountry', 'country', ' nocity']:\n","                    match['city'] = ''\n","                    if lev in ['country']:\n","                        match['subcountry'] = ''\n","                site[s]['match'] = match.to_dict(orient='records')\n","    return site\n","\n","project_site_fix_dict = fix_site(project['site'].to_numpy())\n","project_site_fix = pd.DataFrame.from_dict(project_site_fix_dict, orient='index')\n","write_file(project_site_fix, 'project_site_fix')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WRGxopZqG4c","colab_type":"code","colab":{}},"source":["%%time\n","import xarray as xr\n","import textdistance\n","f = np.vectorize(textdistance.levenshtein.normalized_similarity)\n","\n","site = project['site'].drop_duplicates().tolist()[:10]\n","site.remove('')\n","\n","coords = {'site': site,\n","          'geonameid': world.index,\n","          'part': world.columns}\n","data = xr.DataArray(0.0,\n","                    dims=coords.keys(),\n","                    coords=coords)\n","\n","for part, ser in world.iteritems():\n","    for geonameid, name in ser.iteritems():\n","        print(part, name)\n","        data.loc[:, geonameid, part] = f(site, name)\n","M = data.max(dim=['geonameid','part'])\n","match = data >= M\n","\n","\n","d = dict()\n","for raw in site:\n","    mask = match.loc[raw].to_pandas()\n","    d[raw] = world.where(mask).stack()#to_numpy().ravel()\n","\n","for raw, match in d.items():\n","    print(raw)\n","    print(match)\n","    print()\n","    print()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAW0I7e2LEO_","colab_type":"code","colab":{}},"source":["project['site'] = project['loc']\n","\n","\n","site = project['site'].drop_duplicates().to_frame().set_index('site').drop('')[:10]\n","site['best_score'] = 0.0\n","site['best_matches'] = [[] for _ in site.iterrows()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3L76_nsDQSQj","colab_type":"code","colab":{}},"source":["site"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzuGq_j6ewnx","colab_type":"code","colab":{}},"source":["world = formatter(pd.read_json(f\"{path}/raw/world-cities_json.json\")) \\\n","    .rename(columns={'name':'city'}) \\\n","    [['geonameid', 'city', 'subcountry', 'country']].set_index('geonameid') \\\n","    .sort_values(['country', 'subcountry', 'city'])\n","# world['nocountry'] =    world['city'] + ', ' + world['subcountry']\n","# world['nosubcountry'] = world['city'] + ', '                              + world['country']\n","# world['nocity'] =                              world['subcountry'] + ', ' + world['country']\n","# world['all'] =          world['city'] + ', ' + world['subcountry'] + ', ' + world['country']\n","\n","world.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSI03vUmtuGS","colab_type":"code","colab":{}},"source":["1+1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1qhMNfhnyCk","colab_type":"code","colab":{}},"source":["%%time\n","\n","import textdistance\n","f = np.vectorize(textdistance.levenshtein.normalized_similarity)\n","\n","# Make copy of index from country_raw\n","X = project[['loc']].drop_duplicates().set_index('loc').drop('').sort_index()\n","\n","for x, _ in X.iterrows():\n","    print(x)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrdL9scjnq1_","colab_type":"code","colab":{}},"source":["\n","for idx, x in X.iterrows():\n","\n","\n","\n","\n","for idx, y in world.iterrows():\n","    print(y['country'])\n","\n","\n","    X[y['all']] = f(X.index, y['all'])\n","\n","\n","\n","# country_match.shape\n","match = X.idxmax(axis=1).to_frame().reset_index()\n","# type(match)\n","# match['hit'] = match.iloc[:,0] == match.iloc[:,1]\n","# idx = ~match['hit']\n","# match[idx]\n","match.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xe1paQgyajY0","colab_type":"code","colab":{}},"source":["%%time\n","\n","import textdistance\n","f = np.vectorize(textdistance.levenshtein.normalized_similarity)\n","\n","# Make copy of index from country_raw\n","X = project[['loc']].drop_duplicates().set_index('loc').drop('').sort_index()\n","\n","\n","# for y in world['country'].drop_duplicates():\n","#     X[y] = f(X.index, y)\n","\n","for idx, x in X.iterrows():\n","\n","\n","\n","\n","for idx, y in world.iterrows():\n","    print(y['country'])\n","\n","\n","    X[y['all']] = f(X.index, y['all'])\n","\n","\n","\n","# country_match.shape\n","match = X.idxmax(axis=1).to_frame().reset_index()\n","# type(match)\n","# match['hit'] = match.iloc[:,0] == match.iloc[:,1]\n","# idx = ~match['hit']\n","# match[idx]\n","match.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5BI7vxgmcxN","colab_type":"code","colab":{}},"source":["match.head(100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDQlw7tKir71","colab_type":"code","colab":{}},"source":["world['nosubcountry'].drop_duplicates().shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0nwBdJHhm9J","colab_type":"code","colab":{}},"source":["import textdistance\n","f = np.vectorize(textdistance.levenshtein.normalized_similarity)\n","\n","# Make copy of index from country_raw\n","country_match = country_raw[[]].copy()\n","country_match.head()\n","\n","for y in country_world:\n","    country_match[y] = f(country_match.index, y)\n","country_match.shape\n","country_match.idxmax(axis=1)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-QNKPizoaze","colab_type":"code","colab":{}},"source":["\n","# city_all = pd.read_json(f\"{path}/raw/world-cities_json.json\", dtype=str)\\\n","#     .apply(lambda x: x.str.strip().str.title()).fillna('').replace(regex=r'^.{0,1}$', value='')\\\n","#     .convert_dtypes()\n","\n","# country_raw = pd.concat([project['loc'], project_entity_raw['country']]).value_counts().drop('').sort_index()\n","\n","# %timeit country_all = city_all['country'].unique()\n","# %timeit country_all = city_all['country'].drop_duplicates()\n","country_all = city_all['country'].str.strip().drop_duplicates().sort_values()\n","# type(country_all)\n","country_all"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"07uQPRmAPStX","colab_type":"code","colab":{}},"source":["city.dtypes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"31PbnIh4jzS9","colab_type":"code","colab":{}},"source":["# ! pip install textdistance\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZYrE5PEN4nb","colab_type":"code","colab":{}},"source":["# ! pip install leven\n","# ! pip install StringDist\n","\n","from leven import levenshtein\n","levenshtein('quinten','Quintin')\n","\n","\n","import stringdist\n","# stringdist.levenshtein(['test', 'gh'], 'testing')\n","\n","# r = np.vectorize(stringdist.levenshtein)\n","# r(['bh','quinten'],['Quintin','k'])\n","\n","import textdistance\n","textdistance.hamming.normalized_similarity(['test', 'gh'], 'text')\n","\n","\n","\n","# levenshtein('quinten',['Quintin', 'gth'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Ie54VfXO1h_","colab_type":"code","colab":{}},"source":["idx = city['country'] == \"United States\"\n","\n","city.loc[idx, 'subcountry'].unique\n","\n","city.groupby(['name', 'country'])['subcountry'].count().sort_values()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsTqnsnFtYxM","colab_type":"code","colab":{}},"source":["entity_type_raw\n","write_file(entity_type_raw, '/raw/entity_type_raw')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KAJAOJe7dTH","colab_type":"code","colab":{}},"source":["city = project_entity.groupby(['city', 'country'])['un_idx'].count().drop(['',''])\n","\n","country = pd.concat([project['loc'], project_entity['country']]).value_counts().drop('')\n","# X\n","\n","\n","# X = project['loc']\n","# project_country = X[X != ''].value_counts()\n","\n","# X = project_entity['country']\n","# entity_country = X[X != ''].value_counts()\n","\n","# all_country = entity_country.add(project_country, fill_value=0).astype(int).sort_values()\n","\n","# X = X[X != '']\n","# entity_city = X[X != ''].value_counts()\n","# entity_city\n","# X.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"74hHjhwEY7eg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}